{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr02w7KZjds6",
        "outputId": "df4775c4-4370-4c2c-9ab6-d8320bfc6e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully fetched HTML and saved to page_content.html\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_html(url, output_file):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Successfully fetched HTML and saved to {output_file}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "\n",
        "website_url = \"https://snielebock.github.io/mrpua/Sorter.html\"\n",
        "output_html_file = \"page_content.html\"\n",
        "fetch_html(website_url, output_html_file )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def extract_papers_from_html(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    # Regex to find the JavaScript array 'objects'\n",
        "    match = re.search(r\"const objects = (.*?);\\s*// Function to filter objects\", html_content, re.DOTALL)\n",
        "\n",
        "    papers_data = []\n",
        "    if match:\n",
        "        json_str = match.group(1).strip()\n",
        "        try:\n",
        "            data = json.loads(json_str)\n",
        "            for item in data:\n",
        "                papers_data.append({\n",
        "                    'id': item.get('pID'),\n",
        "                    'title': item.get('Title'),\n",
        "                    'artifact_url': item.get('ArtifactURL'),\n",
        "                    'doi_url': item.get('DOIURL')\n",
        "                })\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON decoding error: {e}\")\n",
        "            # Provide more context for debugging\n",
        "            start = max(0, e.pos - 50)\n",
        "            end = e.pos + 50\n",
        "            print(f\"Problematic string part: ...{json_str[start:end]}...\")\n",
        "    else:\n",
        "        print(\"Could not find the 'objects' JavaScript array in the HTML file.\")\n",
        "\n",
        "    return papers_data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    html_file = \"page_content.html\"\n",
        "    papers = extract_papers_from_html(html_file)\n",
        "    if papers:\n",
        "        print(f\"Successfully extracted {len(papers)} papers.\")\n",
        "        # Save the extracted data to a JSON file for further processing\n",
        "        with open(\"papers_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(papers, f, indent=4)\n",
        "        print(\"Saved paper data to papers_data.json\")\n",
        "    else:\n",
        "        print(\"Failed to extract papers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtIG2nB4ji3g",
        "outputId": "e9297107-b78e-4174-c9a6-24ebab4041c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted 189 papers.\n",
            "Saved paper data to papers_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_download_pdf(doi_url, title):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "        resp = requests.get(doi_url, headers=headers, allow_redirects=True, timeout=20)\n",
        "        # If the final URL is a PDF, or the content-type is PDF\n",
        "        if resp.url.lower().endswith('.pdf') or 'pdf' in resp.headers.get('content-type', '').lower():\n",
        "            pdf_url = resp.url\n",
        "        else:\n",
        "            # Parse the landing page for PDF links\n",
        "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "            pdf_url = None\n",
        "            for a in soup.find_all('a', href=True):\n",
        "                href = a['href']\n",
        "                if '.pdf' in href.lower():\n",
        "                    if href.startswith('/'):\n",
        "                        base_url = '{uri.scheme}://{uri.netloc}'.format(uri=requests.utils.urlparse(resp.url))\n",
        "                        href = base_url + href\n",
        "                    elif not href.startswith('http'):\n",
        "                        href = resp.url.rstrip('/') + '/' + href\n",
        "                    pdf_url = href\n",
        "                    break\n",
        "        if not pdf_url:\n",
        "            print(f\"❌ No PDF found for {doi_url}\")\n",
        "            return None\n",
        "        # Download the PDF\n",
        "        pdf_resp = requests.get(pdf_url, headers=headers, timeout=30)\n",
        "        if pdf_resp.status_code == 200 and 'pdf' in pdf_resp.headers.get('content-type', '').lower():\n",
        "            os.makedirs('pdf', exist_ok=True)\n",
        "            safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:100]\n",
        "            pdf_path = os.path.join('pdf', f\"{safe_title}.pdf\")\n",
        "            with open(pdf_path, 'wb') as f:\n",
        "                f.write(pdf_resp.content)\n",
        "            print(f\"✅ Downloaded: {pdf_path}\")\n",
        "            return pdf_path\n",
        "        else:\n",
        "            print(f\"❌ Failed to download PDF from: {pdf_url}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error for {doi_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Usage example:\n",
        "papers = []\n",
        "try:\n",
        "    with open(\"papers_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        papers = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: papers_data.json not found. Please run the previous steps to generate it.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON from papers_data.json: {e}\")\n",
        "\n",
        "for paper in papers:\n",
        "    title = paper.get(\"title\")\n",
        "    doi_url = paper.get(\"artifact_url\")\n",
        "    if doi_url:\n",
        "        find_and_download_pdf(doi_url, title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6vHMlUVDrbw",
        "outputId": "319b4bd1-e893-4c71-b366-4753a788e35d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ No PDF found for https://github.com/wangdeze18/Multilingual-Adapter-for-SE\n",
            "❌ No PDF found for https://github.com/ZJU-CTAG/CCRep\n",
            "❌ No PDF found for https://github.com/ReliableCoding/REPEAT\n",
            "❌ No PDF found for https://github.com/CGCL-codes/JOpFuzzer\n",
            "❌ No PDF found for https://github.com/lochnagarr/JITFuzz\n",
            "❌ Failed to download PDF from: https://github.com/CGCL-codes/HistFuzz/blob/main/._ICSE_23_HistFuzz.pdf\n",
            "❌ No PDF found for https://github.com/youhanmo/DRFuzz\n",
            "❌ No PDF found for https://github.com/TypeOracle/TypeOracleSrc\n",
            "❌ Error for https://ﬁgshare.com/articles/software/Reproduction_Package_for_Data_Quality_for_Software_Vulnerability_Datasets/20499924: Failed to parse: https://ﬁgshare.com/articles/software/Reproduction_Package_for_Data_Quality_for_Software_Vulnerability_Datasets/20499924\n",
            "❌ No PDF found for https://github.com/gems-uff/refactoring-merge\n",
            "❌ No PDF found for https://github.com/MOB2022/MOB-dataset\n",
            "❌ No PDF found for https://figshare.com/articles/conference_contribution/Debugging_Assumptions_Artifact/21786743\n",
            "❌ No PDF found for https://github.com/hanada31/CrashTracker\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7578677\n",
            "❌ No PDF found for https://github.com/feifeiniu-se/traceability\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7626901\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7528202\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7037673\n",
            "❌ No PDF found for https://github.com/x2018/apicad_public\n",
            "❌ No PDF found for https://github.com/PSDroid2022\n",
            "❌ No PDF found for https://sites.google.com/view/icse2023sca\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.21966875.v1\n",
            "❌ Failed to download PDF from: https://github.com/InPlusLab/ReentrancyStudy-Data/blob/main/materials_to_reentrancy.pdf\n",
            "❌ No PDF found for https://github.com/InPlusLab/bshunter-btcd\n",
            "❌ No PDF found for https://figshare.com/s/5cedff387200ebc379cb\n",
            "❌ No PDF found for https://github.com/TACC-Code/TACC\n",
            "❌ Failed to download PDF from: https://github.com/jun-zeng/Tailor/blob/main/paper.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7625865\n",
            "✅ Downloaded: pdf/Reachable_Coverage__Estimating_Saturation_in_Fuzzing.pdf\n",
            "❌ Failed to download PDF from: https://github.com/kupl/SeamFuzz-public/blob/master/SeamFuzz-accepted-version.pdf\n",
            "❌ No PDF found for https://github.com/GCMiner/GCMiner\n",
            "❌ No PDF found for https://github.com/Tricker-z/CoFuzz\n",
            "❌ No PDF found for https://github.com/cmu-soda/fortis-core\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.21820140\n",
            "❌ Failed to download PDF from: https://github.com/jspaper22/bftdetector/blob/main/BFTDetector__Automatic_Detection_of_Business_Flow_Tampering_for_Digital_Content_Service.pdf\n",
            "❌ Failed to download PDF from: https://zenodo.org/records/7536416/files/icse2023-paper1794.pdf?download=1\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.5442986\n",
            "❌ No PDF found for https://osf.io/s8mhw/?viewonly=42a1f52903964e68836faa76f84a180f\n",
            "❌ No PDF found for https://github.com/lyvd/bad-snakes-icse23-artifacts\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7578656\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7321934\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7577909\n",
            "✅ Downloaded: pdf/Responsibility_in_Context__On_Applicability_of_Slicing_in_Semantic_Regression_Analysis.pdf\n",
            "❌ No PDF found for https://drive.google.com/drive/folders/14Eg4krlQWZO8yrZlWMp325GGn42GtC2h\n",
            "❌ Failed to download PDF from: https://github.com/coinse/fonte/blob/main/preprint.pdf\n",
            "❌ No PDF found for https://github.com/ICSE-2023/RepresentThemALL\n",
            "❌ Failed to download PDF from: https://github.com/ZhangZhuoSJTU/Web3Bugs/blob/main/papers/icse23.pdf\n",
            "❌ Failed to download PDF from: https://raw.githubusercontent.com/wangteng13/Parachute/main/paper/icse23-Parachute.pdf\n",
            "✅ Downloaded: pdf/Explaining_Software_Bugs_Leveraging_Code_Structures_in_Neural_Machine_Translation.pdf\n",
            "❌ No PDF found for https://figshare.com/s/addd697d581c82f96f9a\n",
            "❌ Failed to download PDF from: https://github.com/CelloCorgi/ICSE2023_Psychoactive/blob/main/Consent%20Form.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.6526833\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7520777\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.20736844.v1\n",
            "❌ Failed to download PDF from: https://zenodo.org/records/7573490/files/README.pdf?download=1\n",
            "❌ No PDF found for https://github.com/SpectraSynthesizer\n",
            "❌ No PDF found for https://github.com/SpectraSynthesizer\n",
            "❌ No PDF found for https://github.com/ICSE-DOME/DOME\n",
            "❌ No PDF found for https://github.com/SoftWiser-group/AdvOC\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7042270\n",
            "❌ No PDF found for https://zenodo.org/record/7042943#.YxG\n",
            "❌ No PDF found for https://github.com/senseconcordia/PILARData_ICSE2023.git\n",
            "❌ No PDF found for https://github.com/ginolzh/Variable_Aware_Log_Abstraction\n",
            "❌ No PDF found for https://github.com/senseconcordia/TempoLo-replication-package\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7577795\n",
            "❌ No PDF found for https://github.com/YintongHuo/SemParser\n",
            "❌ No PDF found for https://github.com/ranpku/Badge\n",
            "❌ No PDF found for https://github.com/sidongfeng/AdaT\n",
            "❌ No PDF found for http://github.com/microsoft/codamosa\n",
            "✅ Downloaded: pdf/TAINT_M_INI___Detecting_Flow_of_Sensitive_Data_in_Mini_Programs_with_Static_Taint_Analysis.pdf\n",
            "❌ No PDF found for https://github.com/DependableSystemsLab/Achecker\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.19727050\n",
            "❌ No PDF found for https://github.com/anonymous-dev904/aspect_generation\n",
            "❌ No PDF found for https://github.com/qibinhang/SeaM\n",
            "✅ Downloaded: pdf/P_Y_E_VOLVE___Automating_Frequent_Code_Changes_in_Python_ML_Systems.pdf\n",
            "❌ No PDF found for https://huggingface.co/datasets/Anonymous1234/RNNDecomposition/tree/main\n",
            "❌ No PDF found for https://figshare.com/articles/software/Chronos-ICSE23/22082075\n",
            "❌ Failed to download PDF from: https://github.com/CGCL-codes/MavenEcoSysResearch/blob/main/paper.pdf\n",
            "❌ No PDF found for https://github.com/cristianstaicu/SecBench.js\n",
            "❌ No PDF found for http://doi.org/10.6084/m9.figshare.21922731\n",
            "❌ No PDF found for https://github.com/ewfsdvvr\n",
            "❌ No PDF found for https://github.com/facebookresearch/data\n",
            "✅ Downloaded: pdf/Detecting_Isolation_Bugs_via_Transaction_Oracle_Construction.pdf\n",
            "❌ No PDF found for https://github.com/parasol-aser/smallrace-open-source\n",
            "❌ Failed to download PDF from: https://zenodo.org/records/7607911/files/code_system.pdf?download=1\n",
            "❌ No PDF found for https://github.com/testingautomated-usi/tig-validity-icse23\n",
            "❌ No PDF found for https://github.com/ise-uiuc/NablaFuzz\n",
            "❌ Failed to download PDF from: https://github.com/Lizn-zn/Uncertainty-Alignment/blob/main/DNNRegressionError.pdf\n",
            "✅ Downloaded: pdf/Revisiting_Neuron_Coverage_for_DNN_Testing__A_Layer_Wise_and_Distribution_Aware_Criterion.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7042929\n",
            "❌ No PDF found for https://doi.org/10.7910/DVN/OHMYAK\n",
            "❌ No PDF found for https://sites.google.com/view/cctest-info\n",
            "✅ Downloaded: pdf/KNOD__Domain_Knowledge_Distilled_Tree_Decoder_for_Automated_Program_Repair.pdf\n",
            "❌ No PDF found for https://github.com/norhh/Rete\n",
            "❌ Error for https://gitlab.uni.lu/sezzini/QAssist/: HTTPSConnectionPool(host='gitlab.uni.lu', port=443): Max retries exceeded with url: /sezzini/QAssist/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7ac64e655c90>, 'Connection to gitlab.uni.lu timed out. (connect timeout=20)'))\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7578078\n",
            "❌ No PDF found for https://continuation.passing.style/GenSym\n",
            "❌ No PDF found for https://github.com/EqDAC/EqDACTool\n",
            "❌ No PDF found for https://github.com/franklinbill/Qtypist\n",
            "❌ No PDF found for https://sites.google.com/usc.edu/lotus/home\n",
            "❌ No PDF found for https://github.com/ucsb-seclab/columbus\n",
            "✅ Downloaded: pdf/GameRTS__A_Regression_Testing_Framework_for_Video_Games.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7554407\n",
            "✅ Downloaded: pdf/Impact_of_Code_Language_Models_on_Automated_Program_Repair.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7029404\n",
            "❌ Failed to download PDF from: https://github.com/mxx1219/TENURE/blob/main/paper.pdf\n",
            "✅ Downloaded: pdf/Automated_Repair_of_Programs_from_Large_Language_Models.pdf\n",
            "❌ Failed to download PDF from: https://github.com/icse2023preich/testability-refactoring-patterns/blob/main/Guidelines%20for%20PRs%20labelling.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7545674\n",
            "❌ Failed to download PDF from: https://github.com/UsmanGohar/FairEnsemble/blob/main/EnsembleFairness.pdf\n",
            "❌ Failed to download PDF from: https://github.com/sumonbis/Fairify/blob/main/Appendix-Result.pdf\n",
            "❌ No PDF found for https://github.com/Jirigesi/BGMD_MAPS\n",
            "❌ No PDF found for https://github.com/armanunix/Fairness-testing\n",
            "❌ Failed to download PDF from: https://zenodo.org/records/7932665/files/ATPChecker.pdf?download=1\n",
            "❌ No PDF found for https://github.com/czycurefun/Requirement-Linking-Adversial-Adaptation\n",
            "❌ No PDF found for https://figshare.com/s/edebdcb73def3bdb7cfb\n",
            "❌ No PDF found for https://github.com/apache/incubator-teaclave-java-tee-sdk\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7578926\n",
            "❌ No PDF found for https://github.com/xjtu-enre/DepFCD\n",
            "❌ Failed to download PDF from: https://github.com/xlab-uiuc/uRTS-ae/blob/main/paper.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7455766\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.21950552.v12\n",
            "❌ No PDF found for https://github.com/BEbillionaireUSD/Hades/\n",
            "❌ No PDF found for https://github.com/BEbillionaireUSD/Eadro\n",
            "❌ No PDF found for https://github.com/IntelligentDDS/LogReducer\n",
            "❌ No PDF found for https://github.com/wellido/Aries\n",
            "❌ No PDF found for https://anonymous.4open.science/r/DLCC-4000/\n",
            "❌ No PDF found for https://github.com/ise-uiuc/FASER\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.20526867\n",
            "❌ No PDF found for https://github.com/llylly/RANUM\n",
            "❌ No PDF found for https://github.com/BonanKou/ASSORT-Automatic-Summarization-of-Stack-Overflow-Posts\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7622496\n",
            "❌ No PDF found for https://github.com/Welch123/iFit\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7570960\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7038907\n",
            "❌ No PDF found for https://github.com/abiUni/spatial_code_comprehension\n",
            "❌ No PDF found for https://github.com/ReliableCoding/CREAM\n",
            "❌ No PDF found for https://github.com/DehaiZhao/SeeHow\n",
            "❌ No PDF found for https://github.com/SageSELab/AidUI\n",
            "❌ No PDF found for https://github.com/apicarve/apicarver\n",
            "❌ No PDF found for https://github.com/franklinbill/ArchiDroid\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.20731855.v1\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7575544\n",
            "❌ Failed to download PDF from: https://github.com/FudanSELab/violationTracker/blob/master/ViolationTracker-preprint-ICSE2023.pdf\n",
            "❌ No PDF found for https://github.com/tju-chenyaosuo/MCS\n",
            "❌ Error for https://zenodo.org/record/7553013: HTTPConnectionPool(host='jinshengba.me', port=80): Max retries exceeded with url: /assets/pdf/qpg_icse23.pdf (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7ac64d731510>: Failed to resolve 'jinshengba.me' ([Errno -2] Name or service not known)\"))\n",
            "❌ No PDF found for https://github.com/tcse-iscas/dqetool\n",
            "❌ Failed to download PDF from: https://zenodo.org/records/7504284/files/accepted_paper.pdf?download=1\n",
            "❌ No PDF found for https://zenodo.org/record/7544891\n",
            "✅ Downloaded: pdf/Learning_Deep_Semantics_for_Test_Completion.pdf\n",
            "❌ No PDF found for https://github.com/LJ2lijia/SkCoder\n",
            "❌ No PDF found for https://github.com/NougatCA/FineTuner\n",
            "❌ No PDF found for https://github.com/antonio-mastropaolo/robustness-copilot\n",
            "❌ No PDF found for https://code-recommenders.github.io\n",
            "❌ No PDF found for https://github.com/dslab-epfl/tinynf\n",
            "❌ No PDF found for https://anonymous.4open.science/r/sibyl-884E\n",
            "❌ No PDF found for http://github.com/DeepSoftwareAnalytics/CoCoSoDa\n",
            "❌ No PDF found for https://github.com/tcse-iscas/crashfuzz\n",
            "❌ Failed to download PDF from: https://github.com/kupl/Diver-Artifact/blob/main/paper.pdf\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.20791240\n",
            "❌ No PDF found for https://github.com/deepvd2022/deepvd2022\n",
            "❌ No PDF found for https://github.com/CGCL-codes/VulBG\n",
            "❌ Failed to download PDF from: https://github.com/AMPLE001/AMPLE/blob/main/grap_simplification_rule.pdf\n",
            "❌ No PDF found for https://github.com/WIP2022/DataSampling4DLVD\n",
            "✅ Downloaded: pdf/Incident_aware_Duplicate_Ticket_Aggregation_for_Cloud_Systems.pdf\n",
            "❌ No PDF found for https://anonymous.4open.science/r/llm-testgen-artifact-2753\n",
            "❌ Failed to download PDF from: https://github.com/ucd-plse/On-the-Reproducibility/blob/master/icse-23-reproducibility.pdf\n",
            "❌ No PDF found for https://github.com/wuchiuwong/ContextAwareReproduction\n",
            "❌ No PDF found for https://github.com/sidongfeng/CAPdroid\n",
            "❌ No PDF found for https://github.com/LUH-DBS/Binger/tree/main/DuetCS\n",
            "❌ No PDF found for https://doi.org/10.6084/m9.figshare.19382588.v1\n",
            "❌ No PDF found for http://ariselab.cse.cuhk.edu.hk/projects.html\n",
            "❌ No PDF found for https://sites.google.com/view/fshader/\n",
            "✅ Downloaded: pdf/MorphQ__Metamorphic_Testing_of_the_Qiskit_Quantum_Computing_Platform.pdf\n",
            "❌ No PDF found for https://github.com/RosaliaTufano/impact_pre-training\n",
            "❌ No PDF found for https://github.com/LogIntelligence/LogPPT\n",
            "✅ Downloaded: pdf/Retrieval_Based_Prompt_Selection_for_Code_Related_Few_Shot_Learning.pdf\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7555469\n",
            "❌ No PDF found for https://sites.google.com/view/contrabert\n",
            "❌ No PDF found for https://github.com/DStream-project\n",
            "✅ Downloaded: pdf/_Partial__Program_Dependence_Learning.pdf\n",
            "❌ No PDF found for https://github.com/MirrorTaint/MirrorTaint\n",
            "❌ No PDF found for https://zenodo.org/records/7569854\n",
            "❌ No PDF found for https://sites.google.com/view/icse23remediation\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7348518\n",
            "❌ No PDF found for https://figshare.com/s/840e1fb94bd972829c80\n",
            "❌ No PDF found for https://github.com/yuqiChen94/Causally-Different-Attacks/\n",
            "❌ No PDF found for https://doi.org/10.5281/zenodo.7575582\n",
            "❌ Failed to download PDF from: https://github.com/less-lab-uva/semLidarFuzz/blob/master/Artifact.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: How to import fitz\n",
        "\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KVfQ2tjFb1n",
        "outputId": "ee76b8bb-ed24-4325-e3ae-112c56f7bc38"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use the pdf file to make a score card that looks at Paper Availability\tIs the full paper accessible (open access or behind paywall)?\tOpen access scores higher; paywalled or unavailable scores lower\n",
        "# # # Availability of Code and Software\tIs the source code provided, accessible, and licensed? Is it version-controlled (e.g., GitHub)?\tPresence of public, well-documented, versioned code repository scores higher\n",
        "# # # Availability of Datasets\tAre datasets used in the paper publicly available with clear access instructions?\tPublic, well-documented datasets score higher; proprietary or unavailable datasets score lower\n",
        "# # # Computer Requirements\tAre the hardware and software requirements clearly specified and reasonable?\tClear, reasonable specs (OS, memory, CPU) score higher; vague or unrealistic specs score lower\n",
        "# # # GPU Requirements\tAre GPU or specialized hardware requirements stated?\tExplicit GPU info scores higher; missing or unclear info scores lower\n",
        "# # # Documentation Quality\tQuality and completeness of README, installation guides, usage instructions, API docs\tDetailed, clear, and comprehensive docs score higher\n",
        "# # # Ease of Setup\tHow easy is it to set up and run the code? Are dependencies and environment management handled?\tUse of containers (Docker), environment files, CI/CD pipelines improve score\n",
        "# # # Reproducibility of Results\tCan the results be reproduced using the provided code, data, and instructions?\tVerified reproducibility or artifact badges score highest; no verification scores lowest\n",
        "# # # Overall Rating\tAggregate score reflecting the above criteria\tWeighted sum or qualitative rating (Excellent, Good, Fair, Poor)\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "\n",
        "def analyze_pdf_for_scorecard(pdf_path):\n",
        "    scorecard = {\n",
        "        \"Paper Availability\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"Availability of Code and Software\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"Availability of Datasets\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"Computer Requirements\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"GPU Requirements\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"Documentation Quality\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"Ease of Setup\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"Reproducibility of Results\": {\"score\": 0, \"notes\": \"\"},\n",
        "        \"Overall Rating\": {\"score\": \"N/A\", \"notes\": \"Manual review required\"}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page_num in range(doc.page_count):\n",
        "            text += doc.load_page(page_num).get_text()\n",
        "        doc.close()\n",
        "\n",
        "        # Analyze Paper Availability (already downloaded, implies availability)\n",
        "        scorecard[\"Paper Availability\"][\"score\"] = 1 # Available\n",
        "        scorecard[\"Paper Availability\"][\"notes\"] = \"PDF was successfully downloaded.\"\n",
        "        # Could add logic here to check if it's likely behind a paywall based on content/source if needed\n",
        "\n",
        "        # Analyze Availability of Code and Software\n",
        "        # Look for keywords and URLs indicating code availability\n",
        "        code_keywords = [\"github.com\", \"gitlab.com\", \"bitbucket.org\", \"source code\", \"repository\", \"code base\", \"implementation details\"]\n",
        "        found_code_url = False\n",
        "        for keyword in code_keywords:\n",
        "            if keyword in text.lower():\n",
        "                scorecard[\"Availability of Code and Software\"][\"score\"] = 1 # Found keywords\n",
        "                scorecard[\"Availability of Code and Software\"][\"notes\"] = f\"Found keyword: {keyword}\"\n",
        "                # Further check for specific URLs\n",
        "                urls = re.findall(r'https?://(?:github|gitlab|bitbucket)\\.com/[\\w.-]+/[\\w.-]+', text, re.IGNORECASE)\n",
        "                if urls:\n",
        "                     scorecard[\"Availability of Code and Software\"][\"score\"] = 2 # Found specific URL\n",
        "                     scorecard[\"Availability of Code and Software\"][\"notes\"] = f\"Found potential repository URL(s): {', '.join(urls[:2])}\"\n",
        "                     found_code_url = True\n",
        "                break\n",
        "        if not found_code_url and scorecard[\"Availability of Code and Software\"][\"score\"] == 0:\n",
        "             scorecard[\"Availability of Code and Software\"][\"notes\"] = \"No obvious code availability keywords/URLs found.\"\n",
        "\n",
        "\n",
        "        # Analyze Availability of Datasets\n",
        "        dataset_keywords = [\"dataset\", \"data set\", \"publicly available data\", \"download data\", \"figshare\", \"zenodo\"]\n",
        "        found_dataset_url = False\n",
        "        for keyword in dataset_keywords:\n",
        "            if keyword in text.lower():\n",
        "                scorecard[\"Availability of Datasets\"][\"score\"] = 1 # Found keywords\n",
        "                scorecard[\"Availability of Datasets\"][\"notes\"] = f\"Found keyword: {keyword}\"\n",
        "                # Could add more specific URL checks for data repositories\n",
        "                urls = re.findall(r'https?://(?:figshare|zenodo)\\.com/[\\w./-]+', text, re.IGNORECASE)\n",
        "                if urls:\n",
        "                     scorecard[\"Availability of Datasets\"][\"score\"] = 2 # Found specific URL\n",
        "                     scorecard[\"Availability of Datasets\"][\"notes\"] = f\"Found potential dataset URL(s): {', '.join(urls[:2])}\"\n",
        "                     found_dataset_url = True\n",
        "                break\n",
        "        if not found_dataset_url and scorecard[\"Availability of Datasets\"][\"Datsets\"] == 0:\n",
        "            scorecard[\"Availability of Datasets\"][\"notes\"] = \"No obvious dataset availability keywords/URLs found.\"\n",
        "\n",
        "\n",
        "        # Analyze Computer Requirements\n",
        "        requirements_keywords = [\"requirements\", \"operating system\", \"OS\", \"memory\", \"RAM\", \"CPU\", \"processor\"]\n",
        "        for keyword in requirements_keywords:\n",
        "            if keyword in text:\n",
        "                scorecard[\"Computer Requirements\"][\"score\"] = 1 # Found some requirement info\n",
        "                scorecard[\"Computer Requirements\"][\"notes\"] = f\"Found keyword: {keyword}. Manual check needed for clarity and reasonableness.\"\n",
        "                break\n",
        "        if scorecard[\"Computer Requirements\"][\"score\"] == 0:\n",
        "             scorecard[\"Computer Requirements\"][\"notes\"] = \"No obvious computer requirement keywords found.\"\n",
        "\n",
        "\n",
        "        # Analyze GPU Requirements\n",
        "        gpu_keywords = [\"GPU\", \"graphics card\", \"CUDA\", \"cuDNN\", \"nVidia\", \"RTX\", \"Titan\", \"A100\", \"V100\"]\n",
        "        for keyword in gpu_keywords:\n",
        "            if keyword in text:\n",
        "                scorecard[\"GPU Requirements\"][\"score\"] = 1 # Found GPU info\n",
        "                scorecard[\"GPU Requirements\"][\"notes\"] = f\"Found keyword: {keyword}. Manual check needed for specific requirements.\"\n",
        "                break\n",
        "        if scorecard[\"GPU Requirements\"][\"score\"] == 0:\n",
        "             scorecard[\"GPU Requirements\"][\"notes\"] = \"No obvious GPU requirement keywords found.\"\n",
        "\n",
        "        # Documentation Quality, Ease of Setup, Reproducibility of Results require deeper analysis\n",
        "        # This usually involves cloning the repo, checking documentation files (README, INSTALL),\n",
        "        # attempting to run the code, and checking for reproducibility badges/claims.\n",
        "        # Automating this from just the PDF text is very limited.\n",
        "        scorecard[\"Documentation Quality\"][\"notes\"] = \"Requires analysis of associated code repository documentation.\"\n",
        "        scorecard[\"Ease of Setup\"][\"notes\"] = \"Requires attempting to set up and run the code.\"\n",
        "        scorecard[\"Reproducibility of Results\"][\"notes\"] = \"Requires attempting to reproduce results and checking for badges/claims.\"\n",
        "\n",
        "\n",
        "    except fitz.FileDataError:\n",
        "        scorecard[\"Paper Availability\"][\"score\"] = -1 # Not accessible\n",
        "        scorecard[\"Paper Availability\"][\"notes\"] = \"Could not open the downloaded PDF file.\"\n",
        "        print(f\"Error opening PDF file: {pdf_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while analyzing {pdf_path}: {e}\")\n",
        "\n",
        "    return scorecard\n",
        "\n",
        "# --- Main script to process downloaded PDFs and generate scorecards ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_directory = 'pdf'\n",
        "    scorecards = {}\n",
        "\n",
        "    if not os.path.exists(pdf_directory):\n",
        "        print(f\"PDF directory '{pdf_directory}' not found. Please run the download step first.\")\n",
        "    else:\n",
        "        pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
        "        if not pdf_files:\n",
        "            print(f\"No PDF files found in '{pdf_directory}'. Please ensure downloads were successful.\")\n",
        "        else:\n",
        "            print(f\"Found {len(pdf_files)} PDF files to analyze.\")\n",
        "            for pdf_file in pdf_files:\n",
        "                pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "                print(f\"\\nAnalyzing: {pdf_file}\")\n",
        "                scorecard = analyze_pdf_for_scorecard(pdf_path)\n",
        "                scorecards[pdf_file] = scorecard\n",
        "                # Print a summary of the automated scores\n",
        "                print(\"--- Automated Scorecard Summary ---\")\n",
        "                for criteria, details in scorecard.items():\n",
        "                    print(f\"{criteria}: Score={details['score']}, Notes: {details['notes']}\")\n",
        "\n",
        "            # Optionally, save the scorecards to a JSON file\n",
        "            with open(\"scorecards.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(scorecards, f, indent=4)\n",
        "            print(\"\\nSaved scorecards to scorecards.json\")\n",
        "\n",
        "            # You can now manually review scorecards.json and perform deeper analysis\n",
        "            # for criteria like Documentation Quality, Ease of Setup, and Reproducibility.\n",
        "            # The current automated analysis from PDF text alone is limited.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAVoA_MTAbqx",
        "outputId": "750d0fc8-e804-4033-88df-b63a3805446b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 PDF files to analyze.\n",
            "\n",
            "Analyzing: Learning_Deep_Semantics_for_Test_Completion.pdf\n",
            "An error occurred while analyzing pdf/Learning_Deep_Semantics_for_Test_Completion.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/EngineeringSoftware/teco., https://github.com/javaparser/javaparser.\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Impact_of_Code_Language_Models_on_Automated_Program_Repair.pdf\n",
            "An error occurred while analyzing pdf/Impact_of_Code_Language_Models_on_Automated_Program_Repair.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/lin-tan/clm\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Responsibility_in_Context__On_Applicability_of_Slicing_in_Semantic_Regression_Analysis.pdf\n",
            "An error occurred while analyzing pdf/Responsibility_in_Context__On_Applicability_of_Slicing_in_Semantic_Regression_Analysis.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=1, Notes: Found keyword: github.com\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Revisiting_Neuron_Coverage_for_DNN_Testing__A_Layer-Wise_and_Distribution-Aware_Criterion.pdf\n",
            "An error occurred while analyzing pdf/Revisiting_Neuron_Coverage_for_DNN_Testing__A_Layer-Wise_and_Distribution-Aware_Criterion.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/Yuanyuan-Yuan/NeuraL-Coverage, https://github.com/Yuanyuan-Yuan/NeuraL-Coverage.\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: MorphQ__Metamorphic_Testing_of_the_Qiskit_Quantum_Computing_Platform.pdf\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/Qiskit/qiskit\n",
            "Availability of Datasets: Score=2, Notes: Found potential dataset URL(s): https://figshare.com/s/dd0d4af20fd6e06148a3\n",
            "Computer Requirements: Score=1, Notes: Found keyword: OS. Manual check needed for clarity and reasonableness.\n",
            "GPU Requirements: Score=1, Notes: Found keyword: GPU. Manual check needed for specific requirements.\n",
            "Documentation Quality: Score=0, Notes: Requires analysis of associated code repository documentation.\n",
            "Ease of Setup: Score=0, Notes: Requires attempting to set up and run the code.\n",
            "Reproducibility of Results: Score=0, Notes: Requires attempting to reproduce results and checking for badges/claims.\n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Retrieval_Based_Prompt_Selection_for_Code_Related_Few_Shot_Learning.pdf\n",
            "An error occurred while analyzing pdf/Retrieval_Based_Prompt_Selection_for_Code_Related_Few_Shot_Learning.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/RaRe-Technologies/gensim, https://github.com/prompt-learning/cedar\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Incident_aware_Duplicate_Ticket_Aggregation_for_Cloud_Systems.pdf\n",
            "An error occurred while analyzing pdf/Incident_aware_Duplicate_Ticket_Aggregation_for_Cloud_Systems.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=1, Notes: Found keyword: github.com\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: P_Y_E_VOLVE___Automating_Frequent_Code_Changes_in_Python_ML_Systems.pdf\n",
            "An error occurred while analyzing pdf/P_Y_E_VOLVE___Automating_Frequent_Code_Changes_in_Python_ML_Systems.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/google/pytype\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Automated_Repair_of_Programs_from_Large_Language_Models.pdf\n",
            "An error occurred while analyzing pdf/Automated_Repair_of_Programs_from_Large_Language_Models.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=1, Notes: Found keyword: github.com\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Revisiting_Neuron_Coverage_for_DNN_Testing__A_Layer_Wise_and_Distribution_Aware_Criterion.pdf\n",
            "An error occurred while analyzing pdf/Revisiting_Neuron_Coverage_for_DNN_Testing__A_Layer_Wise_and_Distribution_Aware_Criterion.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/Yuanyuan-Yuan/NeuraL-Coverage, https://github.com/Yuanyuan-Yuan/NeuraL-Coverage.\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Explaining_Software_Bugs_Leveraging_Code_Structures_in_Neural_Machine_Translation.pdf\n",
            "An error occurred while analyzing pdf/Explaining_Software_Bugs_Leveraging_Code_Structures_in_Neural_Machine_Translation.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/scilab/scilab, https://github.com/scilab/scilab\n",
            "Availability of Datasets: Score=0, Notes: \n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: TAINT_M_INI___Detecting_Flow_of_Sensitive_Data_in_Mini-Programs_with_Static_Taint_Analysis.pdf\n",
            "An error occurred while analyzing pdf/TAINT_M_INI___Detecting_Flow_of_Sensitive_Data_in_Mini-Programs_with_Static_Taint_Analysis.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/OSUSecLab/TaintMini.\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: _Partial__Program_Dependence_Learning.pdf\n",
            "An error occurred while analyzing pdf/_Partial__Program_Dependence_Learning.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/deeppda-icse23/DeepPDA\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Reachable_Coverage__Estimating_Saturation_in_Fuzzing.pdf\n",
            "An error occurred while analyzing pdf/Reachable_Coverage__Estimating_Saturation_in_Fuzzing.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/ossf/fuzz-introspector, https://github.com/google/AFL\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: zenodo\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: KNOD__Domain_Knowledge_Distilled_Tree_Decoder_for_Automated_Program_Repair.pdf\n",
            "An error occurred while analyzing pdf/KNOD__Domain_Knowledge_Distilled_Tree_Decoder_for_Automated_Program_Repair.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/c2nes/javalang, https://github.com/javaparser/javaparser\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: TAINT_M_INI___Detecting_Flow_of_Sensitive_Data_in_Mini_Programs_with_Static_Taint_Analysis.pdf\n",
            "An error occurred while analyzing pdf/TAINT_M_INI___Detecting_Flow_of_Sensitive_Data_in_Mini_Programs_with_Static_Taint_Analysis.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/OSUSecLab/TaintMini.\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Detecting_Isolation_Bugs_via_Transaction_Oracle_Construction.pdf\n",
            "An error occurred while analyzing pdf/Detecting_Isolation_Bugs_via_Transaction_Oracle_Construction.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/ept/hermitage, https://github.com/anse1/sqlsmith\n",
            "Availability of Datasets: Score=0, Notes: \n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: GameRTS__A_Regression_Testing_Framework_for_Video_Games.pdf\n",
            "An error occurred while analyzing pdf/GameRTS__A_Regression_Testing_Framework_for_Video_Games.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/fstirlitz/luaparse\n",
            "Availability of Datasets: Score=0, Notes: \n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Incident-aware_Duplicate_Ticket_Aggregation_for_Cloud_Systems.pdf\n",
            "An error occurred while analyzing pdf/Incident-aware_Duplicate_Ticket_Aggregation_for_Cloud_Systems.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=1, Notes: Found keyword: github.com\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Analyzing: Retrieval-Based_Prompt_Selection_for_Code-Related_Few-Shot_Learning.pdf\n",
            "An error occurred while analyzing pdf/Retrieval-Based_Prompt_Selection_for_Code-Related_Few-Shot_Learning.pdf: 'Datsets'\n",
            "--- Automated Scorecard Summary ---\n",
            "Paper Availability: Score=1, Notes: PDF was successfully downloaded.\n",
            "Availability of Code and Software: Score=2, Notes: Found potential repository URL(s): https://github.com/RaRe-Technologies/gensim, https://github.com/prompt-learning/cedar\n",
            "Availability of Datasets: Score=1, Notes: Found keyword: dataset\n",
            "Computer Requirements: Score=0, Notes: \n",
            "GPU Requirements: Score=0, Notes: \n",
            "Documentation Quality: Score=0, Notes: \n",
            "Ease of Setup: Score=0, Notes: \n",
            "Reproducibility of Results: Score=0, Notes: \n",
            "Overall Rating: Score=N/A, Notes: Manual review required\n",
            "\n",
            "Saved scorecards to scorecards.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def scrape_papers_from_sorter(url):\n",
        "    \"\"\"Scrape the JavaScript array of paper metadata from the Sorter.html page.\"\"\"\n",
        "    resp = requests.get(url)\n",
        "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "    script_tag = soup.find('script', string=re.compile(r'const objects = \\['))\n",
        "    if not script_tag:\n",
        "        print(\"❌ Could not find the script tag with paper data.\")\n",
        "        return []\n",
        "    match = re.search(r'const objects = (\\[.*?\\]);', script_tag.string, re.DOTALL)\n",
        "    if not match:\n",
        "        print(\"❌ Could not extract the objects array.\")\n",
        "        return []\n",
        "    try:\n",
        "        papers = json.loads(match.group(1))\n",
        "        return papers\n",
        "    except Exception as e:\n",
        "        print(f\"❌ JSON decode error: {e}\")\n",
        "        return []\n",
        "\n",
        "def find_and_download_pdf(doi_url, title):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    try:\n",
        "        print(f\"🔍 Fetching: {doi_url}\")\n",
        "        resp = requests.get(doi_url, headers=headers, allow_redirects=True, timeout=20)\n",
        "        resp.raise_for_status()\n",
        "        # If the response is a PDF, save it directly\n",
        "        if resp.url.lower().endswith('.pdf') or 'application/pdf' in resp.headers.get('content-type', '').lower():\n",
        "            pdf_url = resp.url\n",
        "        else:\n",
        "            # Parse HTML for PDF links\n",
        "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "            pdf_url = None\n",
        "            # Try meta tag (common in Zenodo, arXiv)\n",
        "            meta_pdf = soup.find('meta', {'name': 'citation_pdf_url'})\n",
        "            if meta_pdf and meta_pdf.get('content'):\n",
        "                pdf_url = meta_pdf['content']\n",
        "            # Try <a> links\n",
        "            if not pdf_url:\n",
        "                for a_tag in soup.find_all('a', href=True):\n",
        "                    href = a_tag['href']\n",
        "                    if '.pdf' in href.lower():\n",
        "                        pdf_url = urljoin(resp.url, href)\n",
        "                        break\n",
        "        if not pdf_url:\n",
        "            print(f\"❌ No PDF found for {doi_url}\")\n",
        "            return None\n",
        "        # Download the PDF\n",
        "        pdf_resp = requests.get(pdf_url, headers=headers, timeout=30)\n",
        "        pdf_resp.raise_for_status()\n",
        "        if 'application/pdf' not in pdf_resp.headers.get('content-type', '').lower():\n",
        "            print(f\"⚠️ Not a PDF at {pdf_url}. Content-Type: {pdf_resp.headers.get('content-type')}\")\n",
        "            return None\n",
        "        os.makedirs('pdf', exist_ok=True)\n",
        "        safe_title = re.sub(r'[^a-zA-Z0-9_\\- ]', '', title).replace(' ', '_')[:100].strip()\n",
        "        if not safe_title:\n",
        "            safe_title = \"downloaded_paper\"\n",
        "        pdf_path = os.path.join('pdf', f\"{safe_title}.pdf\")\n",
        "        with open(pdf_path, 'wb') as f:\n",
        "            f.write(pdf_resp.content)\n",
        "        print(f\"✅ Downloaded: {pdf_path}\")\n",
        "        return pdf_path\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error for {doi_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    sorter_url = \"https://snielebock.github.io/mrpua/Sorter.html\"\n",
        "    papers = scrape_papers_from_sorter(sorter_url)\n",
        "    if not papers:\n",
        "        print(\"No papers found.\")\n",
        "        return\n",
        "    print(f\"Found {len(papers)} papers. Attempting to download PDFs...\")\n",
        "    downloaded_count = 0\n",
        "    for paper in papers:\n",
        "        title = paper.get(\"title\")\n",
        "        # Prefer DOIURL, then ArtifactURL, then ArtifactURL2\n",
        "        url = paper.get(\"doi_url\") or paper.get(\"artifact_url\")\n",
        "        if not url:\n",
        "            print(f\"❌ No valid URL for paper: {title}\")\n",
        "            continue\n",
        "        pdf_path = find_and_download_pdf(doi_url, title)\n",
        "        if pdf_path:\n",
        "            downloaded_count += 1\n",
        "    print(f\"\\n--- Download Summary ---\")\n",
        "    print(f\"Successfully downloaded {downloaded_count} PDFs to the 'pdf' directory.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaXIQLRcsiYv",
        "outputId": "e07679f5-7de6-4c93-d071-aeb6aeee0756"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 189 papers. Attempting to download PDFs...\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "❌ No valid URL for paper: None\n",
            "\n",
            "--- Download Summary ---\n",
            "Successfully downloaded 0 PDFs to the 'pdf' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prompt: make a json file score card that grades the pdf based on Paper Availability\tIs the full paper accessible (open access or behind paywall)?\tOpen access scores higher; paywalled or unavailable scores lower\n",
        "# # Availability of Code and Software\tIs the source code provided, accessible, and licensed? Is it version-controlled (e.g., GitHub)?\tPresence of public, well-documented, versioned code repository scores higher\n",
        "# # Availability of Datasets\tAre datasets used in the paper publicly available with clear access instructions?\tPublic, well-documented datasets score higher; proprietary or unavailable datasets score lower\n",
        "# # Computer Requirements\tAre the hardware and software requirements clearly specified and reasonable?\tClear, reasonable specs (OS, memory, CPU) score higher; vague or unrealistic specs score lower\n",
        "# # GPU Requirements\tAre GPU or specialized hardware requirements stated?\tExplicit GPU info scores higher; missing or unclear info scores lower\n",
        "# # Documentation Quality\tQuality and completeness of README, installation guides, usage instructions, API docs\tDetailed, clear, and comprehensive docs score higher\n",
        "# # Ease of Setup\tHow easy is it to set up and run the code? Are dependencies and environment management handled?\tUse of containers (Docker), environment files, CI/CD pipelines improve score\n",
        "# # Reproducibility of Results\tCan the results be reproduced using the provided code, data, and instructions?\tVerified reproducibility or artifact badges score highest; no verification scores lowest\n",
        "# # Overall Rating\tAggregate score reflecting the above criteria\tWeighted sum or qualitative rating (Excellent, Good, Fair, Poor)\n",
        "\n",
        "# This section focuses on creating the JSON file from the existing scorecard_data DataFrame\n",
        "# Assumes scorecard_df has been created in the preceding code block\n",
        "\n",
        "if 'scorecard_df' in locals() and not scorecard_df:\n",
        "    # Convert the DataFrame to a list of dictionaries (JSON format)\n",
        "    scorecard_json_data = scorecard_df.to_dict(orient='records')\n",
        "\n",
        "    # Define the output JSON file name\n",
        "    scorecard_json_file = \"scorecard.json\"\n",
        "\n",
        "    # Write the data to the JSON file\n",
        "    try:\n",
        "        with open(scorecard_json_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(scorecard_json_data, f, indent=4)\n",
        "        print(f\"\\nScorecard data saved to {scorecard_json_file}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to {scorecard_json_file}: {e}\")\n",
        "else:\n",
        "    print(\"\\nScorecard DataFrame is not available or is empty. Cannot create scorecard.json.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT_Jb0vFPieE",
        "outputId": "c458b979-3971-47d0-e36d-5d1f1a84fe2d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scorecard DataFrame is not available or is empty. Cannot create scorecard.json.\n"
          ]
        }
      ]
    }
  ]
}